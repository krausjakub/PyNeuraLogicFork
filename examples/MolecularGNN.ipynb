{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKVdX_O7ZvY0"
   },
   "source": [
    "# Introduction: GNN for molecules\n",
    "\n",
    "This example will showcase how to define and evaluate a GNN-like model in PyNeuraLogic on a sample molecule classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CEE1Vy1Z5pQ"
   },
   "source": [
    "Install PyNeuraLogic from PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:49.417670Z",
     "start_time": "2024-03-28T14:46:45.621674Z"
    },
    "id": "SujsflJQZ6Nj"
   },
   "outputs": [],
   "source": [
    "! pip install neuralogic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flQw3rmscE7K"
   },
   "source": [
    "## A Relational Dataset\n",
    "\n",
    "We are going to use one of the predefined datasets called [Mutagenesis](https://www.doc.ic.ac.uk/~shm/mutagenesis.html). This popular dataset contains information about molecules that we are going to classify for mutagenicity on Salmonella typhimurium.\n",
    "\n",
    "Predefined datasets are located in `neuralogic.utils.data`, and by calling `Mutagenesis`, we retrieve a tuple containing the input data (dataset) as well as a default template (rules). For this example, we are going to ignore the template as we will practice defining our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:49.958668Z",
     "start_time": "2024-03-28T14:46:49.421677Z"
    },
    "id": "mglV4jndaL2B"
   },
   "outputs": [],
   "source": [
    "from neuralogic.utils.data import Mutagenesis\n",
    "\n",
    "_, dataset = Mutagenesis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk8AIcCFasFV"
   },
   "source": [
    "Predefined examples are loaded from a file. We can check the first sample to get an idea how the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:49.974670Z",
     "start_time": "2024-03-28T14:46:49.961671Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VaRsqkIasaZ",
    "outputId": "e0c391ff-139e-4ead-8a5a-fe458f508580"
   },
   "outputs": [],
   "source": [
    "with open(dataset.examples_file) as fp:\n",
    "    print(fp.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLfOk69ublG9"
   },
   "source": [
    "As can be seen, the molecules are essentially encoded as heterogeneous graphs with edges called `bonds`, where the first two terms are the chemical atom ids, and the third term is the bond id. The atoms' ids are then associated with specific atom types, such as `h` for hydrogen. Bond ids are then associated with bond types, such as `b_1` for a single bond, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Note that this textual storage format is just a bit more compact version of the way we would encode the data directly in the PyNeuraLogic language (Python), where we would instead write:\n",
    "\n",
    "```\n",
    "Relation.bond(Term.d59_23, Term.d59_5, 0), Relation.h(Term.d59_23), ...\n",
    "```\n",
    "\n",
    "Next we take a look at the associated *queries* which are the target labels associated with the examples in supervised learning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.006671Z",
     "start_time": "2024-03-28T14:46:49.978676Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(dataset.queries_file) as fp:\n",
    "    print(fp.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "we can see that each example molecule is associated with just a simple binary label called 'predict' here.\n",
    "\n",
    "<sup>\n",
    "* Note that in general you can associate each example with multiple different queries (e.g. joint classification), and that these can also be structured.\n",
    "<sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOZev_cScMQy"
   },
   "source": [
    "## A Relational Template\n",
    "\n",
    "The specification of the learning template is now up to the user. This is where you can get creative, but any valid template has to somehow connect the input example representations with the output queries (i.e. the 'predict' label).\n",
    "In this example, we are going to first turn the atom and bond types into embeddings, and then propagate these jointly through the molecular graphs in a GNN-like fashion.\n",
    "\n",
    "So let's start by embedding each chemical atom type into an `atom_embed` relation with a unique learnable parameter vector of shape `[3]` (a 3-dimensional embedding).\n",
    "Doing this for each type can be simplified with list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.054673Z",
     "start_time": "2024-03-28T14:46:50.034674Z"
    },
    "id": "vf-ryNvOcOYX"
   },
   "outputs": [],
   "source": [
    "from neuralogic.core import Template, R, V\n",
    "\n",
    "template = Template()\n",
    "\n",
    "template.add_rules([\n",
    "    (R.atom_embed(V.A)[3,] <= R.get(atom)(V.A)) for atom in [\"c\", \"o\", \"br\", \"i\", \"f\", \"h\", \"n\", \"cl\"]\n",
    "])\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV58Qp-JcVlu"
   },
   "source": [
    "In the same way, we can encode the embeddings for the bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.085669Z",
     "start_time": "2024-03-28T14:46:50.061670Z"
    },
    "id": "wuliEkxzcWZJ"
   },
   "outputs": [],
   "source": [
    "template.add_rules([\n",
    "    (R.bond_embed(V.B)[3,] <= R.get(bond)(V.B)) for bond in [\"b_1\", \"b_2\", \"b_3\", \"b_4\", \"b_5\", \"b_7\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMI5Icgrccum"
   },
   "source": [
    "Next we add the GNN-like graph propagation rule itself.\n",
    "Particularly, we will want to project representations of all the nodes (`atom_embed`) through a learnable matrix of shape `[3, 3]`.\n",
    "We will do this separately for the \"central\" node `X` and its neighbors `Y` (nodes connected by the `bond` relation) to accommodate for their different roles here.\n",
    "Since we also have different bond types here, we will combine these atom projections together with the bond embedding.\n",
    "For each central node `X`, we will then aggregate these representations from all the corresponding neighbors `Y`, to serve as the new representation for the central node `X`.\n",
    "Let's call this resulting representation `layer_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.119672Z",
     "start_time": "2024-03-28T14:46:50.090670Z"
    },
    "id": "YAGhZIy9cdcg"
   },
   "outputs": [],
   "source": [
    "template +=  R.layer_1(V.X) <= (R.atom_embed(V.X)[3, 3], R.atom_embed(V.Y)[3, 3], R.bond(V.X, V.Y, V.B), R.bond_embed(V.B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA5ehjCNcnlu"
   },
   "source": [
    "In the same way, we can now add some more \"layers\" that will be 'connected' by utilizing the previously defined representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.149695Z",
     "start_time": "2024-03-28T14:46:50.124672Z"
    },
    "id": "df_vNmbwcoIF"
   },
   "outputs": [],
   "source": [
    "template +=  R.layer_2(V.X) <= (R.layer_1(V.X)[3, 3], R.layer_1(V.Y)[3, 3], R.bond(V.X, V.Y, V.B), R.bond_embed(V.B))\n",
    "template +=  R.layer_3(V.X) <= (R.layer_2(V.X)[3, 3], R.layer_2(V.Y)[3, 3], R.bond(V.X, V.Y, V.B), R.bond_embed(V.B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZhNNrfrcuw7"
   },
   "source": [
    "Finally, to obatin a single label for the whole graph, we then aggregate all the nodes' (atoms') representations from the last layer (`layer_3`), and project them through another learnable vector of shape `[1, 3]` into a scalar value.\n",
    "We will call the resulting representation `predict` to correspond to the learning target queries given in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:50.181671Z",
     "start_time": "2024-03-28T14:46:50.155671Z"
    },
    "id": "K29q8NvLcvq4"
   },
   "outputs": [],
   "source": [
    "template += R.predict[1, 3] <= R.layer_3(V.X)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the template visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:52.394675Z",
     "start_time": "2024-03-28T14:46:50.191673Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uju8LDYwcy2m"
   },
   "source": [
    "## Training\n",
    "\n",
    "When we have our dataset and template ready, it's time to build (\"ground\") the template over the dataset and start training.\n",
    "We can do the training manually and write our own custom training loop, but we can also use predefined helpers - *evaluators*,\n",
    "that handle model and dataset building, training, and more. Evaluators can be customized via `Settings`.\n",
    "\n",
    "<sup>Note that building the dataset (=grounding the logic rules and translating into neural networks) may take a while, depending on the complexity of your template.\n",
    "But this is only done once before the training itself, which takes up most of the time anyway.\n",
    "<sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:46:59.516669Z",
     "start_time": "2024-03-28T14:46:52.397670Z"
    },
    "id": "OvixrLnZc2Ao"
   },
   "outputs": [],
   "source": [
    "from neuralogic.core import Settings\n",
    "from neuralogic.nn.loss import MSE, CrossEntropy\n",
    "from neuralogic.nn import get_evaluator\n",
    "from neuralogic.optim import Adam\n",
    "\n",
    "settings = Settings(optimizer=Adam(lr=0.001), epochs=100, error_function=MSE())\n",
    "evaluator = get_evaluator(template, settings)\n",
    "\n",
    "built_dataset = evaluator.build_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-AH9fwFddSn"
   },
   "source": [
    "Finally, we iterate through the iterator encapsulated in the `train` method of the evaluator, which yields a total loss of the epoch and the number of samples of the current epoch.\n",
    "We then get access to the results from the training loop that we can further visualize, inspect, log, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sSVqIDvoddzV",
    "outputId": "9b4ed322-e895-4005-8f03-5e6486b159c7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_losses = []\n",
    "\n",
    "for current_total_loss, number_of_samples in evaluator.train(built_dataset):\n",
    "    clear_output(wait=True)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.xlim(0, settings.epochs)\n",
    "\n",
    "    average_losses.append(current_total_loss / number_of_samples)\n",
    "    \n",
    "    plt.plot(average_losses, label=\"Average loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl4xOLjP0vOG"
   },
   "source": [
    "We can then check the trained model predictions (for the same sample set here) by utilizing the `test` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:47:36.701798Z",
     "start_time": "2024-03-28T14:47:36.657799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrRL7-wk0t_L",
    "outputId": "9f62a6ce-b29d-4d1a-947f-eb16f5d93683",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for sample, y_hat in zip(built_dataset.samples, evaluator.test(built_dataset)):\n",
    "    print(f\"Target: {sample.java_sample.target}, Predicted: {round(y_hat)} ({y_hat})\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mutagenesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
